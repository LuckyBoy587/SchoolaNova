{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuckyBoy587/SchoolaNova/blob/main/BERT_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESaxZ80KkEB2"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# 1. Load embedding model (pretrained on similarity tasks)\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "# 2. Sample NCERT-like paragraphs\n",
        "docs = [\n",
        "    \"Photosynthesis is the process by which green plants make food using sunlight, carbon dioxide, and water.\",\n",
        "    \"Respiration is the process by which living organisms release energy from food molecules.\",\n",
        "    \"Chlorophyll is a green pigment in plants that helps capture sunlight for photosynthesis.\"\n",
        "]\n",
        "\n",
        "# 3. Encode the paragraphs\n",
        "doc_embeddings = model.encode(docs, convert_to_tensor=True)\n",
        "\n",
        "# 4. Encode a sample question\n",
        "question = \"How do plants cook themselves?\"\n",
        "query_embedding = model.encode(question, convert_to_tensor=True)\n",
        "\n",
        "# 5. Compute similarity\n",
        "cosine_scores = util.cos_sim(query_embedding, doc_embeddings)\n",
        "print(cosine_scores)\n",
        "\n",
        "# 6. Retrieve best paragraph\n",
        "best_idx = cosine_scores.argmax()\n",
        "print(\"Question:\", question)\n",
        "print(\"Best Match:\", docs[best_idx])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIX_AZIxsGjq"
      },
      "outputs": [],
      "source": [
        "def split_into_chunks(text):\n",
        "    return text.splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MofOR6usHsB"
      },
      "outputs": [],
      "source": [
        "file_chunks = []\n",
        "with open(\"/content/drive/MyDrive/ColabContent/photosynthesis.txt\") as f:\n",
        "  file_chunks = split_into_chunks(f.read())\n",
        "  print(*file_chunks, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thjXJWejs9gn"
      },
      "outputs": [],
      "source": [
        "query = \"Where did the name photosysnthesis came from?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxOAeq_kCPio"
      },
      "outputs": [],
      "source": [
        "chunk_embeddings = model.encode(file_chunks, convert_to_tensor=True)\n",
        "query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "cosine_scores = util.cos_sim(query_embedding, chunk_embeddings)\n",
        "best_idx = cosine_scores.argmax()\n",
        "\n",
        "print(\"Question:\", query)\n",
        "print(\"Best Match:\", file_chunks[best_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6iAaWUOJzld"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AotDV3TWJv_X"
      },
      "outputs": [],
      "source": [
        "import pdfplumber, re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Aggressively clean NCERT/School-book style PDF text with OCR noise.\"\"\"\n",
        "    # Normalize spaces/newlines\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove OCR garbled \"CChhaapptteerr\" like strings\n",
        "    text = re.sub(r'C+H*A+P+T+E+R+.*?\\d+', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove lines with .indd and timestamps (OCR file tags)\n",
        "    text = re.sub(r'\\.?i+n+d+d+\\s*\\d+.*?(AM|PM)?', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove publisher footer lines\n",
        "    text = re.sub(r'Curiosity.*?(Grade|Gr\\.a\\.d\\.e)', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove \"Chapter ...\" repeated headers (even broken ones)\n",
        "    text = re.sub(r'Chapter\\s+The.*?Solutions', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove random numbers/dates like //2288//22002255 ::0066::3366\n",
        "    text = re.sub(r'[/:\\d]+', '', text)\n",
        "\n",
        "    # Collapse multiple punctuation (.... → . , ??? → ? , !!! → !)\n",
        "    text = re.sub(r'([.?!])\\1+', r'\\1', text)\n",
        "\n",
        "    # Remove bullets/list markers\n",
        "    text = re.sub(r'[\\\\•\\·\\●\\-\\–\\—\\»\\\"z]', '', text)\n",
        "\n",
        "    # Normalize ligatures\n",
        "    text = text.replace('ﬁ', 'fi').replace('ﬂ', 'fl')\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def pdf_to_sentences(pdf_path: str):\n",
        "    \"\"\"Extract sentences from PDF and clean them.\"\"\"\n",
        "    all_sentences = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            raw_text = page.extract_text()\n",
        "            if not raw_text:\n",
        "                continue\n",
        "            # Clean and split into sentences\n",
        "            cleaned = clean_text(raw_text)\n",
        "            sentences = sent_tokenize(cleaned)\n",
        "            for s in sentences:\n",
        "                s = s.strip()\n",
        "                if len(s.split()) > 5:  # skip very tiny fragments\n",
        "                    all_sentences.append(s)\n",
        "    return all_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTfdi3Lknnqi"
      },
      "outputs": [],
      "source": [
        "print(*pdf_to_sentences(\"/content/drive/MyDrive/ColabContent/ch9.pdf\"), sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abfe2459"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch9_sentences = pdf_to_sentences(\"/content/drive/MyDrive/ColabContent/ch9.pdf\")"
      ],
      "metadata": {
        "id": "pcVJEr2fzQSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "ch9_embeddings = model.encode(ch9_sentences, convert_to_tensor=False) # Convert to numpy array for FAISS\n",
        "\n",
        "# create a FAISS index\n",
        "d = ch9_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(ch9_embeddings)\n",
        "\n",
        "print(f\"Created FAISS index with {index.ntotal} vectors.\")"
      ],
      "metadata": {
        "id": "V9MWK8v3yD7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_question = \"What is a solute and a solvent?\"\n",
        "my_question_embd = model.encode(my_question, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "wrUFwBRQyOvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Encode the question and convert to numpy\n",
        "my_question_embd_np = model.encode([my_question], convert_to_tensor=False)\n",
        "\n",
        "# Search the FAISS index\n",
        "top_k = 5\n",
        "distances, indices = index.search(my_question_embd_np, top_k)\n",
        "\n",
        "print(\"Question:\", my_question)\n",
        "print(f\"Top {top_k} Matches:\")\n",
        "for i in range(top_k):\n",
        "    print(f\"- {ch9_sentences[indices[0][i]]}\")"
      ],
      "metadata": {
        "id": "WtIgUexGzDi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c9d3491"
      },
      "source": [
        "!pip install faiss-cpu"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOioENu0tDFdHjPuMztpZOT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}