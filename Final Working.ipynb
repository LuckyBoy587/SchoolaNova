{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1WOy1Of6UimkVFHC9YNtJkJRZD2kP2d0S",
      "authorship_tag": "ABX9TyNB97LCjJ0DEeTOOtjy4om7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuckyBoy587/SchoolaNova/blob/main/Final%20Working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c626183"
      },
      "source": [
        "%pip install pdfplumber nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdhEC5jijKZF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from typing import List, Dict, Any, Iterable, Pattern, Union, Optional\n",
        "import pdfplumber\n",
        "import nltk\n",
        "import os # Added os import for file path handling\n",
        "import glob # Added glob import for file path pattern matching\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Extra cleaning (your rules)\n",
        "# -------------------------\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Aggressively clean NCERT/School-book style PDF text with OCR noise.\"\"\"\n",
        "    # Normalize spaces/newlines\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove OCR garbled \"CChhaapptteerr\" like strings\n",
        "    text = re.sub(r'C+H*A+P+T+E+R+.*?\\d+', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove lines with .indd and timestamps (OCR file tags)\n",
        "    text = re.sub(r'\\.?i+n+d+d+\\s*\\d+.*?(AM|PM)?', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove publisher footer lines\n",
        "    text = re.sub(r'Curiosity.*?(Grade|Gr\\.a\\.d\\.e)', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove \"Chapter ...\" repeated headers (even broken ones)\n",
        "    text = re.sub(r'Chapter\\s+The.*?Solutions', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Define the core tokens once\n",
        "    fig_core = r'(?:Fig\\.?|Figure)\\s*:?\\s*\\d+(?:[.\\-\\u2013]\\d+)*(?:[A-Za-z])?(?:\\([A-Za-z]\\))?'\n",
        "    tab_core = r'(?:Tab\\.?|Table)\\s*:?\\s*\\d+(?:[.\\-\\u2013]\\d+)*(?:[A-Za-z])?(?:\\([A-Za-z]\\))?'\n",
        "\n",
        "    # 1) Remove unbracketed inline refs like \"Fig. 9.10a\" or \"Table 4.1,\"\n",
        "    text = re.sub(rf'\\b(?:{fig_core}|{tab_core})(?:\\s*[:.,;])?', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # 2) Remove bracketed inline refs like \"(Fig. 9.10a)\" or \"(Figure 3(b))\"\n",
        "    text = re.sub(rf'\\(\\s*(?:{fig_core}|{tab_core})\\s*\\)(?:\\s*[:.,;])?', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # 3) Remove full caption lines starting with these tokens (with or without brackets)\n",
        "    text = re.sub(rf'(?mi)^\\s*(?:\\(\\s*)?(?:{fig_core}|{tab_core})(?:\\s*\\))?\\s+.*$', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # 1) Dates like 12/10/2021 or 12-10-21\n",
        "    text = re.sub(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b', '', text)\n",
        "\n",
        "    # 2) Time like 12:30 or 12:30:45\n",
        "    text = re.sub(r'\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\b', '', text)\n",
        "\n",
        "    # 3) Weird ::0066::3366 blobs (keep the colons, drop number blobs between them)\n",
        "    text = re.sub(r'::\\d+::', '::', text)\n",
        "\n",
        "    # 4) Standalone page-like counters with lots of slashes (//2288// style)\n",
        "    text = re.sub(r'/{2,}\\d+/{2,}', ' ', text)\n",
        "\n",
        "    # 5) Hyphenated index codes like 2020-001-33 at line edges (optional, be careful)\n",
        "    text = re.sub(r'\\b\\d{4}-\\d{1,3}-\\d{1,3}\\b', '', text)\n",
        "\n",
        "    # Collapse multiple punctuation (.... → . , ??? → ? , !!! → !)\n",
        "    text = re.sub(r'([.?!])\\1+', r'\\1', text)\n",
        "\n",
        "    text = re.sub(r\"\\(\\)\", \"\", text)\n",
        "\n",
        "    # Remove bullets/list markers (normalized to valid escapes; kept your 'z' if it's an OCR bullet)\n",
        "    text = re.sub(r'[\\x8b•·●\\-\\–\\—»\"z]', '', text)\n",
        "\n",
        "    # Normalize ligatures\n",
        "    text = text.replace('ﬁ', 'fi').replace('ﬂ', 'fl')\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Layout helpers\n",
        "# -------------------------\n",
        "def words_to_lines(words: List[Dict[str, Any]], line_tol: float = 3.0):\n",
        "    \"\"\"\n",
        "    Group word dicts (from pdfplumber.extract_words) into visual lines by y (top) coordinate.\n",
        "    Returns a list of {'text','x0','x1','top','bottom'} for each line.\n",
        "    \"\"\"\n",
        "    if not words:\n",
        "        return []\n",
        "\n",
        "    words_sorted = sorted(words, key=lambda w: (w[\"top\"], w[\"x0\"]))\n",
        "    lines: List[Dict[str, Any]] = []\n",
        "\n",
        "    current: List[Dict[str, Any]] = [words_sorted[0]]\n",
        "    current_top = words_sorted[0][\"top\"]\n",
        "\n",
        "    for w in words_sorted[1:]:\n",
        "        if abs(w[\"top\"] - current_top) <= line_tol:\n",
        "            current.append(w)\n",
        "        else:\n",
        "            lines.append(_make_line(current))\n",
        "            current = [w]\n",
        "            current_top = w[\"top\"]\n",
        "\n",
        "    if current:\n",
        "        lines.append(_make_line(current))\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def _make_line(ws: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    ws_sorted = sorted(ws, key=lambda w: w[\"x0\"])\n",
        "    text = \" \".join(w[\"text\"] for w in ws_sorted)\n",
        "    return {\n",
        "        \"text\": re.sub(r\"\\s+\", \" \", text).strip(),\n",
        "        \"x0\": min(w[\"x0\"] for w in ws_sorted),\n",
        "        \"x1\": max(w[\"x1\"] for w in ws_sorted),\n",
        "        \"top\": min(w[\"top\"] for w in ws_sorted),\n",
        "        \"bottom\": max(w[\"bottom\"] for w in ws_sorted),\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Rules and regexes\n",
        "# -------------------------\n",
        "# Caption detector (used only if you choose to drop caption lines early)\n",
        "CAPTION_RE = re.compile(\n",
        "    r\"^\\s*(?:\\(\\s*)?(?:fig(?:ure)?|tab(?:le)?)\\.?\\s*\\d+(?:[.\\-–]\\d+)*(?:[A-Za-z])?(?:\\([A-Za-z]\\))?\\s*[:\\-–)]?\\s*\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "# End-of-sentence punctuation (optionally closed by quotes/paren)\n",
        "END_PUNCT_RE = re.compile(r'[.!?][\"”’)\\]]?$')\n",
        "\n",
        "# Exceptions like \"Fig. 10.2\", \"Sec. 2.3\", \"No. 5.1\" (do not treat as sentence end)\n",
        "ABBR_BEFORE_NUMBER_RE = re.compile(r\"\\b(?:fig|sec|no|vol|ch|pg|pp)\\.\\s*\\d\", re.IGNORECASE)\n",
        "\n",
        "\n",
        "def _compile_patterns(patterns: Iterable[Union[str, Pattern]]) -> List[Pattern]:\n",
        "    out = []\n",
        "    for p in patterns:\n",
        "        if isinstance(p, str):\n",
        "            out.append(re.compile(p, re.IGNORECASE))\n",
        "        else:\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "\n",
        "def _chapter_name_to_pattern(name: str) -> Pattern:\n",
        "    \"\"\"\n",
        "    Turn a literal chapter name into a robust regex that matches with flexible punctuation/whitespace.\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r\"\\w+\", name, flags=re.UNICODE)\n",
        "    if not tokens:\n",
        "        return re.compile(re.escape(name), re.IGNORECASE)\n",
        "    pattern = r\"\\b\" + r\"[\\s\\W]*\".join(re.escape(tok) for tok in tokens) + r\"\\b\"\n",
        "    return re.compile(pattern, re.IGNORECASE)\n",
        "\n",
        "\n",
        "def _build_banned_res(\n",
        "    banned_line_patterns: Iterable[Union[str, Pattern]],\n",
        "    chapter_names: Optional[Iterable[str]] = None,\n",
        ") -> List[Pattern]:\n",
        "    res = _compile_patterns(banned_line_patterns)\n",
        "    if chapter_names:\n",
        "        for name in chapter_names:\n",
        "            if name and name.strip():\n",
        "                res.append(_chapter_name_to_pattern(name.strip()))\n",
        "    return res\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Main extractor\n",
        "# -------------------------\n",
        "def extract_sentences_in_y_range(\n",
        "    pdf_path: str,\n",
        "    y_min: float = 100,\n",
        "    y_max: float = 700,\n",
        "    line_tol: float = 3.0,\n",
        "    banned_line_patterns: Iterable[Union[str, Pattern]] = (r\"\\bgrade\\s*8\\b\",),\n",
        "    chapter_names: Optional[Iterable[str]] = None,\n",
        "    remove_strategy: str = \"strip\",   # \"strip\" (remove matches within the line) or \"drop\" (drop whole line)\n",
        "    drop_caption_lines: bool = True,  # drop lines that look like figure/table captions\n",
        "    apply_extra_cleaning: bool = True, # apply clean_text() on flushed sentences\n",
        "    prune_meaningful: bool = False    # NEW: if True, prune sentences that look meaningless/noisy\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Extract sentences with layout-aware logic and aggressive cleaning.\n",
        "\n",
        "    Returns list of:\n",
        "      {\n",
        "        \"page_number\": int,\n",
        "        \"text\": str,\n",
        "        \"type\": \"body\",\n",
        "        \"bbox\": [x0, top, x1, bottom]\n",
        "      }\n",
        "    Note: when drop_caption_lines=True, captions are removed; if False, they are emitted as type=\"caption\".\n",
        "    \"\"\"\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    banned_res = _build_banned_res(banned_line_patterns, chapter_names)\n",
        "\n",
        "    def cleanse_line_text(line_text: str) -> str:\n",
        "        \"\"\"Remove banned substrings; collapse whitespace.\"\"\"\n",
        "        t = line_text\n",
        "        for pat in banned_res:\n",
        "            t = pat.sub(\" \", t)\n",
        "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "        return t\n",
        "\n",
        "    def line_contains_banned(line_text: str) -> bool:\n",
        "        return any(p.search(line_text) for p in banned_res)\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_idx, page in enumerate(pdf.pages):  # page_idx now starts from 0\n",
        "            words = page.extract_words(\n",
        "                x_tolerance=2,\n",
        "                y_tolerance=2,\n",
        "                keep_blank_chars=False,\n",
        "                use_text_flow=True,\n",
        "            )\n",
        "\n",
        "            # Filter by y-range first\n",
        "            words = [w for w in words if y_min <= float(w[\"top\"]) <= y_max]\n",
        "            if not words:\n",
        "                continue\n",
        "\n",
        "            lines = words_to_lines(words, line_tol=line_tol)\n",
        "            if not lines:\n",
        "                continue\n",
        "\n",
        "            processed_lines: List[Dict[str, Any]] = []\n",
        "            for ln in lines:\n",
        "                raw_text = ln[\"text\"]\n",
        "                if not raw_text:\n",
        "                    continue\n",
        "\n",
        "                # Drop caption lines early if requested\n",
        "                if drop_caption_lines and CAPTION_RE.match(raw_text):\n",
        "                    continue\n",
        "\n",
        "                # Banned text removal/drop on a per-line basis\n",
        "                if remove_strategy == \"drop\":\n",
        "                    if line_contains_banned(raw_text):\n",
        "                        continue\n",
        "                    new_text = raw_text\n",
        "                else:\n",
        "                    new_text = cleanse_line_text(raw_text)\n",
        "                    if not new_text:\n",
        "                        continue\n",
        "\n",
        "                processed_lines.append(\n",
        "                    {\n",
        "                        \"text\": new_text,\n",
        "                        \"x0\": ln[\"x0\"],\n",
        "                        \"x1\": ln[\"x1\"],\n",
        "                        \"top\": ln[\"top\"],\n",
        "                        \"bottom\": ln[\"bottom\"],\n",
        "                        \"page_number\": page_idx,  # Store page number for each line\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            if not processed_lines:\n",
        "                continue\n",
        "\n",
        "            # Build sentences across lines\n",
        "            buffer_text = \"\"\n",
        "            buffer_bbox = None  # bbox of accumulated lines\n",
        "            buffer_page_number = None  # page number for the current buffer\n",
        "\n",
        "            def flush_buffer(_type=\"body\"):\n",
        "                nonlocal buffer_text, buffer_bbox, buffer_page_number\n",
        "                s = buffer_text.strip()\n",
        "                if not s:\n",
        "                    buffer_text = \"\"\n",
        "                    buffer_bbox = None\n",
        "                    buffer_page_number = None\n",
        "                    return\n",
        "\n",
        "                # Final cleaning at sentence level\n",
        "                if apply_extra_cleaning:\n",
        "                    s = clean_text(s)\n",
        "                    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "                if s:\n",
        "                    results.append(\n",
        "                        {\n",
        "                            \"page_number\": buffer_page_number if buffer_page_number is not None else 0,\n",
        "                            \"text\": s,\n",
        "                            \"type\": _type,\n",
        "                            \"bbox\": buffer_bbox if buffer_bbox else [0, 0, 0, 0],\n",
        "                        }\n",
        "                    )\n",
        "                buffer_text = \"\"\n",
        "                buffer_bbox = None\n",
        "                buffer_page_number = None\n",
        "\n",
        "            for ln in processed_lines:\n",
        "                text = ln[\"text\"]\n",
        "                if not text:\n",
        "                    continue\n",
        "\n",
        "                # Hyphenated line-break fix: if buffer ends with \"-\", merge without space\n",
        "                if buffer_text and buffer_text.endswith(\"-\"):\n",
        "                    buffer_text = buffer_text[:-1] + text.lstrip()\n",
        "                else:\n",
        "                    buffer_text = (buffer_text + \" \" + text).strip() if buffer_text else text\n",
        "\n",
        "                # Grow bbox\n",
        "                if buffer_bbox is None:\n",
        "                    buffer_bbox = [ln[\"x0\"], ln[\"top\"], ln[\"x1\"], ln[\"bottom\"]]\n",
        "                else:\n",
        "                    buffer_bbox = [\n",
        "                        min(buffer_bbox[0], ln[\"x0\"]),\n",
        "                        min(buffer_bbox[1], ln[\"top\"]),\n",
        "                        max(buffer_bbox[2], ln[\"x1\"]),\n",
        "                        max(buffer_bbox[3], ln[\"bottom\"]),\n",
        "                    ]\n",
        "\n",
        "                # Track page number for the buffer (use the first line's page number)\n",
        "                if buffer_page_number is None:\n",
        "                    buffer_page_number = ln[\"page_number\"]\n",
        "\n",
        "                # Sentence boundary detection on the (line-level) text\n",
        "                if END_PUNCT_RE.search(text):\n",
        "                    tail = text[-12:]\n",
        "                    if not ABBR_BEFORE_NUMBER_RE.search(tail):\n",
        "                        flush_buffer(\"body\")\n",
        "\n",
        "            # Flush trailing text on the page\n",
        "            flush_buffer(\"body\")\n",
        "\n",
        "    if prune_meaningful:\n",
        "        filtered = [r for r in results if _is_meaningful_sentence(r[\"text\"])]\n",
        "        return filtered\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def _is_meaningful_sentence(s: str, min_words: int = 3, min_alpha_chars: int = 3) -> bool:\n",
        "    \"\"\"\n",
        "    Heuristic checks to prune short/noisy OCR lines that carry no meaningful content.\n",
        "    Rules:\n",
        "      - must have at least `min_words` words (simple token split)\n",
        "      - must contain at least `min_alpha_chars` alphabetic characters\n",
        "      - must not be punctuation/ellipsis only\n",
        "      - must not be timestamp-like or repetition like '151 PM PM'\n",
        "      - must not have a very high punctuation ratio\n",
        "    \"\"\"\n",
        "    if not s:\n",
        "        return False\n",
        "\n",
        "    s_stripped = s.strip()\n",
        "    # drop lines that are just punctuation or repeated dots\n",
        "    punct_class = re.escape(string.punctuation)\n",
        "    if re.fullmatch(fr'^[\\s{punct_class}]+$', s_stripped):\n",
        "        return False\n",
        "\n",
        "    words = [w for w in re.split(r'\\s+', s_stripped) if w]\n",
        "    if len(words) < min_words:\n",
        "        return False\n",
        "\n",
        "    alpha_chars = len(re.findall(r'[A-Za-z]', s_stripped))\n",
        "    if alpha_chars < min_alpha_chars:\n",
        "        return False\n",
        "\n",
        "    # punctuation ratio\n",
        "    punct_chars = len(re.findall(r'[^\\w\\s]', s_stripped))\n",
        "    if len(s_stripped) > 0 and (punct_chars / max(1, len(s_stripped)) > 0.6):\n",
        "        return False\n",
        "\n",
        "    # timestamp-like or repeated AM/PM noise (e.g., \"151 PM PM\" or \"12:30 PM\")\n",
        "    if re.search(r'\\b\\d{1,3}(:\\d{2})?\\s*(?:AM|PM)\\b', s_stripped, flags=re.IGNORECASE):\n",
        "        # if there's more alphabetic text around the timestamp it's ok; otherwise drop\n",
        "        # consider it noise if AM/PM occurs >=2 times or if only digits+AM/PM present\n",
        "        ampm_count = len(re.findall(r'\\b(?:AM|PM)\\b', s_stripped, flags=re.IGNORECASE))\n",
        "        if ampm_count >= 2 or re.fullmatch(r'^[\\d\\s:APMapm.]+$', s_stripped):\n",
        "            return False\n",
        "\n",
        "    # common short OCR fragments and single-character noise\n",
        "    if re.fullmatch(r'[\\.\\-]+', s_stripped):\n",
        "        return False\n",
        "\n",
        "    # drop lines with excessive digit-only content\n",
        "    digit_chars = len(re.findall(r'\\d', s_stripped))\n",
        "    if digit_chars > 0 and (digit_chars / max(1, len(s_stripped)) > 0.6) and alpha_chars < 2:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def extract_metadata_from_pdf(path: str, names_to_ignore: list[str]) -> List[str]:\n",
        "    # Adjust y-range to exclude headers/footers if needed\n",
        "    y_min, y_max = 20, 1000\n",
        "    sentences = extract_sentences_in_y_range(\n",
        "        path,\n",
        "        y_min=y_min,\n",
        "        y_max=y_max,\n",
        "        line_tol=3.0,\n",
        "        banned_line_patterns=names_to_ignore,\n",
        "        chapter_names=names_to_ignore[:1],\n",
        "        remove_strategy=\"strip\",\n",
        "        drop_caption_lines=True,\n",
        "        apply_extra_cleaning=True,\n",
        "        prune_meaningful=True,   # enable pruning of non-meaningful lines\n",
        "    )\n",
        "\n",
        "    return chunk_text_list([s[\"text\"] for s in sentences])\n",
        "\n",
        "def chunk_text_list(text_list: List[str], n=100) -> List[str]:\n",
        "    \"\"\"\n",
        "    Takes a list of strings (e.g., book pages/paragraphs),\n",
        "    cleans them, concatenates, and chunks into ~n-word blocks.\n",
        "    Returns a list of chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except:\n",
        "        nltk.download('punkt')\n",
        "\n",
        "    # Join all input strings into one big text\n",
        "    full_text = \" \".join(text_list)\n",
        "\n",
        "    sentences = nltk.sent_tokenize(full_text)\n",
        "\n",
        "    chunks, cur_chunk = [], \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        cur_chunk += \" \" + sentence\n",
        "        if len(cur_chunk.split()) >= n:\n",
        "            chunks.append(cur_chunk.strip())\n",
        "            cur_chunk = \"\"\n",
        "\n",
        "    if cur_chunk:\n",
        "        chunks.append(cur_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Example usage\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_path = r\"/content/drive/MyDrive/ColabContent/ch9.pdf\"  # Replace with your PDF file path\n",
        "\n",
        "    chapter_names = [\n",
        "        \"9.1 What Are Solute, Solvent, and Solution?\",\n",
        "    ]\n",
        "\n",
        "    # Extend banned patterns as needed (hidden grade/class marks, roman etc.)\n",
        "    banned = [\n",
        "        r\"\\bgrade\\s*8\\b\",\n",
        "        r\"\\bclass\\s*8\\b\",\n",
        "        r\"\\bclass\\s*viii\\b\",\n",
        "    ]\n",
        "\n",
        "    sentences = extract_metadata_from_pdf(pdf_path, names_to_ignore=banned)\n",
        "\n",
        "    for s in sentences:\n",
        "        print(s)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zgaFcpKdj1sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a248a68a"
      },
      "source": [
        "**Note**: You may need to restart the kernel after installing the libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from typing import List, Dict, Any, Iterable, Pattern, Union, Optional\n",
        "import pdfplumber\n",
        "import nltk\n",
        "import os # Added os import for file path handling\n",
        "import glob # Added glob import for file path pattern matching\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import tempfile\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Extra cleaning (your rules)\n",
        "# -------------------------\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Aggressively clean NCERT/School-book style PDF text with OCR noise.\"\"\"\n",
        "    # Normalize spaces/newlines\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove OCR garbled \"CChhaapptteerr\" like strings\n",
        "    text = re.sub(r'C+H*A+P+T+E+R+.*?\\d+', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove lines with .indd and timestamps (OCR file tags)\n",
        "    text = re.sub(r'\\.?i+n+d+d+\\s*\\d+.*?(AM|PM)?', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove publisher footer lines\n",
        "    text = re.sub(r'Curiosity.*?(Grade|Gr\\.a\\.d\\.e)', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove \"Chapter ...\" repeated headers (even broken ones)\n",
        "    text = re.sub(r'Chapter\\s+The.*?Solutions', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Define the core tokens once\n",
        "    fig_core = r'(?:Fig\\.?|Figure)\\s*:?\\s*\\d+(?:[.\\-\\u2013]\\d+)*(?:[A-Za-z])?(?:\\([A-Za-z]\\))?'\n",
        "    tab_core = r'(?:Tab\\.?|Table)\\s*:?\\s*\\d+(?:[.\\-\\u2013]\\d+)*(?:[A-Za-z])?(?:\\([A-Za-z]\\))?'\n",
        "\n",
        "    # 1) Remove unbracketed inline refs like \"Fig. 9.10a\" or \"Table 4.1,\"\n",
        "    text = re.sub(rf'\\b(?:{fig_core}|{tab_core})(?:\\s*[:.,;])?', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # 2) Remove bracketed inline refs like \"(Fig. 9.10a)\" or \"(Figure 3(b))\"\n",
        "    text = re.sub(rf'\\(\\s*(?:{fig_core}|{tab_core})\\s*\\)(?:\\s*[:.,;])?', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # 3) Remove full caption lines starting with these tokens (with or without brackets)\n",
        "    text = re.sub(rf'(?mi)^\\s*(?:\\(\\s*)?(?:{fig_core}|{tab_core})(?:\\s*\\))?\\s+.*$', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # 1) Dates like 12/10/2021 or 12-10-21\n",
        "    text = re.sub(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b', '', text)\n",
        "\n",
        "    # 2) Time like 12:30 or 12:30:45\n",
        "    text = re.sub(r'\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\b', '', text)\n",
        "\n",
        "    # 3) Weird ::0066::3366 blobs (keep the colons, drop number blobs between them)\n",
        "    text = re.sub(r'::\\d+::', '::', text)\n",
        "\n",
        "    # 4) Standalone page-like counters with lots of slashes (//2288// style)\n",
        "    text = re.sub(r'/{2,}\\d+/{2,}', ' ', text)\n",
        "\n",
        "    # 5) Hyphenated index codes like 2020-001-33 at line edges (optional, be careful)\n",
        "    text = re.sub(r'\\b\\d{4}-\\d{1,3}-\\d{1,3}\\b', '', text)\n",
        "\n",
        "    # Collapse multiple punctuation (.... → . , ??? → ? , !!! → !)\n",
        "    text = re.sub(r'([.?!])\\1+', r'\\1', text)\n",
        "\n",
        "    text = re.sub(r\"\\(\\)\", \"\", text)\n",
        "\n",
        "    # Remove bullets/list markers (normalized to valid escapes; kept your 'z' if it's an OCR bullet)\n",
        "    text = re.sub(r'[\\x8b•·●\\-\\–\\—»\"z]', '', text)\n",
        "\n",
        "    # Normalize ligatures\n",
        "    text = text.replace('ﬁ', 'fi').replace('ﬂ', 'fl')\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Layout helpers\n",
        "# -------------------------\n",
        "def words_to_lines(words: List[Dict[str, Any]], line_tol: float = 3.0):\n",
        "    \"\"\"\n",
        "    Group word dicts (from pdfplumber.extract_words) into visual lines by y (top) coordinate.\n",
        "    Returns a list of {'text','x0','x1','top','bottom'} for each line.\n",
        "    \"\"\"\n",
        "    if not words:\n",
        "        return []\n",
        "\n",
        "    words_sorted = sorted(words, key=lambda w: (w[\"top\"], w[\"x0\"]))\n",
        "    lines: List[Dict[str, Any]] = []\n",
        "\n",
        "    current: List[Dict[str, Any]] = [words_sorted[0]]\n",
        "    current_top = words_sorted[0][\"top\"]\n",
        "\n",
        "    for w in words_sorted[1:]:\n",
        "        if abs(w[\"top\"] - current_top) <= line_tol:\n",
        "            current.append(w)\n",
        "        else:\n",
        "            lines.append(_make_line(current))\n",
        "            current = [w]\n",
        "            current_top = w[\"top\"]\n",
        "\n",
        "    if current:\n",
        "        lines.append(_make_line(current))\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def _make_line(ws: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    ws_sorted = sorted(ws, key=lambda w: w[\"x0\"])\n",
        "    text = \" \".join(w[\"text\"] for w in ws_sorted)\n",
        "    return {\n",
        "        \"text\": re.sub(r\"\\s+\", \" \", text).strip(),\n",
        "        \"x0\": min(w[\"x0\"] for w in ws_sorted),\n",
        "        \"x1\": max(w[\"x1\"] for w in ws_sorted),\n",
        "        \"top\": min(w[\"top\"] for w in ws_sorted),\n",
        "        \"bottom\": max(w[\"bottom\"] for w in ws_sorted),\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Rules and regexes\n",
        "# -------------------------\n",
        "# Caption detector (used only if you choose to drop caption lines early)\n",
        "CAPTION_RE = re.compile(\n",
        "    r\"^\\s*(?:\\(\\s*)?(?:fig(?:ure)?|tab(?:le)?)\\.?\\s*\\d+(?:[.\\-–]\\d+)*(?:[A-Za-z])?(?:\\([A-Za-z]\\))?\\s*[:\\-–)]?\\s*\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "# End-of-sentence punctuation (optionally closed by quotes/paren)\n",
        "END_PUNCT_RE = re.compile(r'[.!?][\"”’)\\]]?$')\n",
        "\n",
        "# Exceptions like \"Fig. 10.2\", \"Sec. 2.3\", \"No. 5.1\" (do not treat as sentence end)\n",
        "ABBR_BEFORE_NUMBER_RE = re.compile(r\"\\b(?:fig|sec|no|vol|ch|pg|pp)\\.\\s*\\d\", re.IGNORECASE)\n",
        "\n",
        "\n",
        "def _compile_patterns(patterns: Iterable[Union[str, Pattern]]) -> List[Pattern]:\n",
        "    out = []\n",
        "    for p in patterns:\n",
        "        if isinstance(p, str):\n",
        "            out.append(re.compile(p, re.IGNORECASE))\n",
        "        else:\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "\n",
        "def _chapter_name_to_pattern(name: str) -> Pattern:\n",
        "    \"\"\"\n",
        "    Turn a literal chapter name into a robust regex that matches with flexible punctuation/whitespace.\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r\"\\w+\", name, flags=re.UNICODE)\n",
        "    if not tokens:\n",
        "        return re.compile(re.escape(name), re.IGNORECASE)\n",
        "    pattern = r\"\\b\" + r\"[\\s\\W]*\".join(re.escape(tok) for tok in tokens) + r\"\\b\"\n",
        "    return re.compile(pattern, re.IGNORECASE)\n",
        "\n",
        "\n",
        "def _build_banned_res(\n",
        "    banned_line_patterns: Iterable[Union[str, Pattern]],\n",
        "    chapter_names: Optional[Iterable[str]] = None,\n",
        ") -> List[Pattern]:\n",
        "    res = _compile_patterns(banned_line_patterns)\n",
        "    if chapter_names:\n",
        "        for name in chapter_names:\n",
        "            if name and name.strip():\n",
        "                res.append(_chapter_name_to_pattern(name.strip()))\n",
        "    return res\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Main extractor\n",
        "# -------------------------\n",
        "def extract_sentences_in_y_range(\n",
        "    pdf_path: str,\n",
        "    y_min: float = 100,\n",
        "    y_max: float = 700,\n",
        "    line_tol: float = 3.0,\n",
        "    banned_line_patterns: Iterable[Union[str, Pattern]] = (r\"\\bgrade\\s*8\\b\",),\n",
        "    chapter_names: Optional[Iterable[str]] = None,\n",
        "    remove_strategy: str = \"strip\",   # \"strip\" (remove matches within the line) or \"drop\" (drop whole line)\n",
        "    drop_caption_lines: bool = True,  # drop lines that look like figure/table captions\n",
        "    apply_extra_cleaning: bool = True, # apply clean_text() on flushed sentences\n",
        "    prune_meaningful: bool = False    # NEW: if True, prune sentences that look meaningless/noisy\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Extract sentences with layout-aware logic and aggressive cleaning.\n",
        "\n",
        "    Returns list of:\n",
        "      {\n",
        "        \"page_number\": int,\n",
        "        \"text\": str,\n",
        "        \"type\": \"body\",\n",
        "        \"bbox\": [x0, top, x1, bottom]\n",
        "      }\n",
        "    Note: when drop_caption_lines=True, captions are removed; if False, they are emitted as type=\"caption\".\n",
        "    \"\"\"\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    banned_res = _build_banned_res(banned_line_patterns, chapter_names)\n",
        "\n",
        "    def cleanse_line_text(line_text: str) -> str:\n",
        "        \"\"\"Remove banned substrings; collapse whitespace.\"\"\"\n",
        "        t = line_text\n",
        "        for pat in banned_res:\n",
        "            t = pat.sub(\" \", t)\n",
        "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "        return t\n",
        "\n",
        "    def line_contains_banned(line_text: str) -> bool:\n",
        "        return any(p.search(line_text) for p in banned_res)\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_idx, page in enumerate(pdf.pages):  # page_idx now starts from 0\n",
        "            words = page.extract_words(\n",
        "                x_tolerance=2,\n",
        "                y_tolerance=2,\n",
        "                keep_blank_chars=False,\n",
        "                use_text_flow=True,\n",
        "            )\n",
        "\n",
        "            # Filter by y-range first\n",
        "            words = [w for w in words if y_min <= float(w[\"top\"]) <= y_max]\n",
        "            if not words:\n",
        "                continue\n",
        "\n",
        "            lines = words_to_lines(words, line_tol=line_tol)\n",
        "            if not lines:\n",
        "                continue\n",
        "\n",
        "            processed_lines: List[Dict[str, Any]] = []\n",
        "            for ln in lines:\n",
        "                raw_text = ln[\"text\"]\n",
        "                if not raw_text:\n",
        "                    continue\n",
        "\n",
        "                # Drop caption lines early if requested\n",
        "                if drop_caption_lines and CAPTION_RE.match(raw_text):\n",
        "                    continue\n",
        "\n",
        "                # Banned text removal/drop on a per-line basis\n",
        "                if remove_strategy == \"drop\":\n",
        "                    if line_contains_banned(raw_text):\n",
        "                        continue\n",
        "                    new_text = raw_text\n",
        "                else:\n",
        "                    new_text = cleanse_line_text(raw_text)\n",
        "                    if not new_text:\n",
        "                        continue\n",
        "\n",
        "                processed_lines.append(\n",
        "                    {\n",
        "                        \"text\": new_text,\n",
        "                        \"x0\": ln[\"x0\"],\n",
        "                        \"x1\": ln[\"x1\"],\n",
        "                        \"top\": ln[\"top\"],\n",
        "                        \"bottom\": ln[\"bottom\"],\n",
        "                        \"page_number\": page_idx,  # Store page number for each line\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            if not processed_lines:\n",
        "                continue\n",
        "\n",
        "            # Build sentences across lines\n",
        "            buffer_text = \"\"\n",
        "            buffer_bbox = None  # bbox of accumulated lines\n",
        "            buffer_page_number = None  # page number for the current buffer\n",
        "\n",
        "            def flush_buffer(_type=\"body\"):\n",
        "                nonlocal buffer_text, buffer_bbox, buffer_page_number\n",
        "                s = buffer_text.strip()\n",
        "                if not s:\n",
        "                    buffer_text = \"\"\n",
        "                    buffer_bbox = None\n",
        "                    buffer_page_number = None\n",
        "                    return\n",
        "\n",
        "                # Final cleaning at sentence level\n",
        "                if apply_extra_cleaning:\n",
        "                    s = clean_text(s)\n",
        "                    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "                if s:\n",
        "                    results.append(\n",
        "                        {\n",
        "                            \"page_number\": buffer_page_number if buffer_page_number is not None else 0,\n",
        "                            \"text\": s,\n",
        "                            \"type\": _type,\n",
        "                            \"bbox\": buffer_bbox if buffer_bbox else [0, 0, 0, 0],\n",
        "                        }\n",
        "                    )\n",
        "                buffer_text = \"\"\n",
        "                buffer_bbox = None\n",
        "                buffer_page_number = None\n",
        "\n",
        "            for ln in processed_lines:\n",
        "                text = ln[\"text\"]\n",
        "                if not text:\n",
        "                    continue\n",
        "\n",
        "                # Hyphenated line-break fix: if buffer ends with \"-\", merge without space\n",
        "                if buffer_text and buffer_text.endswith(\"-\"):\n",
        "                    buffer_text = buffer_text[:-1] + text.lstrip()\n",
        "                else:\n",
        "                    buffer_text = (buffer_text + \" \" + text).strip() if buffer_text else text\n",
        "\n",
        "                # Grow bbox\n",
        "                if buffer_bbox is None:\n",
        "                    buffer_bbox = [ln[\"x0\"], ln[\"top\"], ln[\"x1\"], ln[\"bottom\"]]\n",
        "                else:\n",
        "                    buffer_bbox = [\n",
        "                        min(buffer_bbox[0], ln[\"x0\"]),\n",
        "                        min(buffer_bbox[1], ln[\"top\"]),\n",
        "                        max(buffer_bbox[2], ln[\"x1\"]),\n",
        "                        max(buffer_bbox[3], ln[\"bottom\"]),\n",
        "                    ]\n",
        "\n",
        "                # Track page number for the buffer (use the first line's page number)\n",
        "                if buffer_page_number is None:\n",
        "                    buffer_page_number = ln[\"page_number\"]\n",
        "\n",
        "                # Sentence boundary detection on the (line-level) text\n",
        "                if END_PUNCT_RE.search(text):\n",
        "                    tail = text[-12:]\n",
        "                    if not ABBR_BEFORE_NUMBER_RE.search(tail):\n",
        "                        flush_buffer(\"body\")\n",
        "\n",
        "            # Flush trailing text on the page\n",
        "            flush_buffer(\"body\")\n",
        "\n",
        "    if prune_meaningful:\n",
        "        filtered = [r for r in results if _is_meaningful_sentence(r[\"text\"])]\n",
        "        return filtered\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def _is_meaningful_sentence(s: str, min_words: int = 3, min_alpha_chars: int = 3) -> bool:\n",
        "    \"\"\"\n",
        "    Heuristic checks to prune short/noisy OCR lines that carry no meaningful content.\n",
        "    Rules:\n",
        "      - must have at least `min_words` words (simple token split)\n",
        "      - must contain at least `min_alpha_chars` alphabetic characters\n",
        "      - must not be punctuation/ellipsis only\n",
        "      - must not be timestamp-like or repetition like '151 PM PM'\n",
        "      - must not have a very high punctuation ratio\n",
        "    \"\"\"\n",
        "    if not s:\n",
        "        return False\n",
        "\n",
        "    s_stripped = s.strip()\n",
        "    # drop lines that are just punctuation or repeated dots\n",
        "    punct_class = re.escape(string.punctuation)\n",
        "    if re.fullmatch(fr'^[\\s{punct_class}]+$', s_stripped):\n",
        "        return False\n",
        "\n",
        "    words = [w for w in re.split(r'\\s+', s_stripped) if w]\n",
        "    if len(words) < min_words:\n",
        "        return False\n",
        "\n",
        "    alpha_chars = len(re.findall(r'[A-Za-z]', s_stripped))\n",
        "    if alpha_chars < min_alpha_chars:\n",
        "        return False\n",
        "\n",
        "    # punctuation ratio\n",
        "    punct_chars = len(re.findall(r'[^\\w\\s]', s_stripped))\n",
        "    if len(s_stripped) > 0 and (punct_chars / max(1, len(s_stripped)) > 0.6):\n",
        "        return False\n",
        "\n",
        "    # timestamp-like or repeated AM/PM noise (e.g., \"151 PM PM\" or \"12:30 PM\")\n",
        "    if re.search(r'\\b\\d{1,3}(:\\d{2})?\\s*(?:AM|PM)\\b', s_stripped, flags=re.IGNORECASE):\n",
        "        # if there's more alphabetic text around the timestamp it's ok; otherwise drop\n",
        "        # consider it noise if AM/PM occurs >=2 times or if only digits+AM/PM present\n",
        "        ampm_count = len(re.findall(r'\\b(?:AM|PM)\\b', s_stripped, flags=re.IGNORECASE))\n",
        "        if ampm_count >= 2 or re.fullmatch(r'^[\\d\\s:APMapm.]+$', s_stripped):\n",
        "            return False\n",
        "\n",
        "    # common short OCR fragments and single-character noise\n",
        "    if re.fullmatch(r'[\\.\\-]+', s_stripped):\n",
        "        return False\n",
        "\n",
        "    # drop lines with excessive digit-only content\n",
        "    digit_chars = len(re.findall(r'\\d', s_stripped))\n",
        "    if digit_chars > 0 and (digit_chars / max(1, len(s_stripped)) > 0.6) and alpha_chars < 2:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def extract_metadata_from_pdf(path: str, names_to_ignore: list[str]) -> List[str]:\n",
        "    # Adjust y-range to exclude headers/footers if needed\n",
        "    y_min, y_max = 20, 1000\n",
        "    sentences = extract_sentences_in_y_range(\n",
        "        path,\n",
        "        y_min=y_min,\n",
        "        y_max=y_max,\n",
        "        line_tol=3.0,\n",
        "        banned_line_patterns=names_to_ignore,\n",
        "        chapter_names=names_to_ignore[:1],\n",
        "        remove_strategy=\"strip\",\n",
        "        drop_caption_lines=True,\n",
        "        apply_extra_cleaning=True,\n",
        "        prune_meaningful=True,   # enable pruning of non-meaningful lines\n",
        "    )\n",
        "\n",
        "    return chunk_text_list([s[\"text\"] for s in sentences])\n",
        "\n",
        "def chunk_text_list(text_list: List[str], n=100) -> List[str]:\n",
        "    \"\"\"\n",
        "    Takes a list of strings (e.g., book pages/paragraphs),\n",
        "    cleans them, concatenates, and chunks into ~n-word blocks.\n",
        "    Returns a list of chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except:\n",
        "        nltk.download('punkt')\n",
        "\n",
        "    # Join all input strings into one big text\n",
        "    full_text = \" \".join(text_list)\n",
        "\n",
        "    sentences = nltk.sent_tokenize(full_text)\n",
        "\n",
        "    chunks, cur_chunk = [], \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        cur_chunk += \" \" + sentence\n",
        "        if len(cur_chunk.split()) >= n:\n",
        "            chunks.append(cur_chunk.strip())\n",
        "            cur_chunk = \"\"\n",
        "\n",
        "    if cur_chunk:\n",
        "        chunks.append(cur_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# sentence-transformers\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    SentenceTransformer = None  # type: ignore\n",
        "\n",
        "# optional acceleration: FAISS CPU/GPU\n",
        "try:\n",
        "    import faiss  # type: ignore\n",
        "    _HAS_FAISS = True\n",
        "    _HAS_FAISS_GPU = hasattr(faiss, \"StandardGpuResources\")\n",
        "except Exception:\n",
        "    faiss = None  # type err: ignore\n",
        "    _HAS_FAISS = False\n",
        "    _HAS_FAISS_GPU = False\n",
        "\n",
        "\n",
        "def _safe_model_name(name: str) -> str:\n",
        "    return name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
        "\n",
        "\n",
        "def _pdf_fingerprint(pdf: str) -> Dict[str, Any]:\n",
        "    st = os.stat(pdf)\n",
        "    return {\"size\": st.st_size, \"mtime\": st.st_mtime}\n",
        "\n",
        "\n",
        "def _chunks_hash(text_chunks: List[str]) -> str:\n",
        "    sep = \"\\n\\u241F\\n\"\n",
        "    return hashlib.sha256(sep.join(text_chunks).encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
        "\n",
        "\n",
        "def _query_hash(q: str) -> str:\n",
        "    return hashlib.sha256(q.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
        "\n",
        "\n",
        "def _get_cache_dir() -> Path:\n",
        "    xdg = os.environ.get(\"XDG_CACHE_HOME\")\n",
        "    if xdg:\n",
        "        return Path(xdg) / \"semantic_search\"\n",
        "    return Path.home() / \".cache\" / \"semantic_search\"\n",
        "\n",
        "\n",
        "def _atomic_write_bytes(path: Path, data: bytes) -> None:\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with tempfile.NamedTemporaryFile(dir=str(path.parent), delete=False) as tf:\n",
        "        tf.write(data)\n",
        "        tmp = Path(tf.name)\n",
        "    tmp.replace(path)\n",
        "\n",
        "\n",
        "def semantic_search_pdf(\n",
        "    pdf_path: str,\n",
        "    user_query: str,\n",
        "    top_k: int = 5,\n",
        "    model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    names_to_ignore: Optional[List[str]] = None,\n",
        "    device: Optional[str] = None,\n",
        "    enable_query_cache: bool = True,\n",
        "    use_faiss: bool = True,\n",
        "    use_faiss_gpu: bool = True,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Semantic search over PDF content using FAISS with optional GPU acceleration.\n",
        "\n",
        "    - Uses SentenceTransformers for embeddings (normalize_embeddings=True so dot product == cosine).\n",
        "    - Builds & persists a FAISS CPU index and metadata; optionally moves index to GPU at runtime.\n",
        "    - Falls back to CPU FAISS or numpy dot-product if faiss is not installed.\n",
        "    - Query embeddings can be cached.\n",
        "\n",
        "    Parameters:\n",
        "    - use_faiss: attempt to use FAISS if available.\n",
        "    - use_faiss_gpu: if FAISS has GPU support, move index to GPU for search.\n",
        "    \"\"\"\n",
        "    model = None\n",
        "    if SentenceTransformer is not None:\n",
        "        try:\n",
        "            # Prefer CUDA device if use_faiss_gpu is True and CUDA is available\n",
        "            model_device = device or (\"cuda\" if use_faiss_gpu and _HAS_FAISS_GPU else None)\n",
        "            model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=model_device)\n",
        "        except Exception:\n",
        "             # Fallback to CPU if CUDA fails or is not available\n",
        "            model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "    if names_to_ignore is None:\n",
        "        names_to_ignore = []\n",
        "\n",
        "    pdf_path = os.path.abspath(pdf_path)\n",
        "    if not os.path.isfile(pdf_path):\n",
        "        raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n",
        "\n",
        "\n",
        "    safe_model = _safe_model_name(model_name)\n",
        "    stem = Path(pdf_path).stem\n",
        "\n",
        "\n",
        "    # --- Load chunks ---\n",
        "    chunks = extract_metadata_from_pdf(pdf_path, names_to_ignore=names_to_ignore)\n",
        "    if not chunks:\n",
        "        return []\n",
        "\n",
        "    fingerprint = _pdf_fingerprint(pdf_path)\n",
        "    chunks_digest = _chunks_hash(chunks)\n",
        "\n",
        "\n",
        "    # --- If no cache, compute embeddings ---\n",
        "    chunk_embeddings = model.encode(\n",
        "        chunks,\n",
        "        convert_to_tensor=False,\n",
        "        show_progress_bar=False,\n",
        "        normalize_embeddings=True,\n",
        "    )\n",
        "    chunk_embeddings_np = np.ascontiguousarray(np.asarray(chunk_embeddings, dtype=np.float32))\n",
        "\n",
        "\n",
        "    num_chunks = chunk_embeddings_np.shape[0]\n",
        "    if num_chunks == 0:\n",
        "        return []\n",
        "\n",
        "    # --- FAISS index loading/building ---\n",
        "    faiss_index = None\n",
        "    faiss_on_gpu = False\n",
        "\n",
        "    if use_faiss and _HAS_FAISS:\n",
        "        try:\n",
        "            dim = int(chunk_embeddings_np.shape[1])\n",
        "            cpu_index = faiss.IndexFlatIP(dim)  # inner product for normalized embeddings\n",
        "            # Ensure contiguous float32\n",
        "            vectors = np.ascontiguousarray(chunk_embeddings_np.astype(np.float32))\n",
        "            cpu_index.add(vectors)\n",
        "\n",
        "            # optionally move to GPU\n",
        "            if use_faiss_gpu and _HAS_FAISS_GPU:\n",
        "                try:\n",
        "                    res = faiss.StandardGpuResources()\n",
        "                    faiss_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
        "                    faiss_on_gpu = True\n",
        "                except Exception:\n",
        "                    faiss_index = cpu_index\n",
        "                    faiss_on_gpu = False\n",
        "            else:\n",
        "                faiss_index = cpu_index\n",
        "                faiss_on_gpu = False\n",
        "        except Exception:\n",
        "            faiss_index = None\n",
        "\n",
        "    # --- Query caching (by hash) --- # Removed query caching logic\n",
        "    query_embedding_np: Optional[np.ndarray] = None\n",
        "    # query_cache: Dict[str, np.ndarray] = {}\n",
        "    qhash = _query_hash(user_query)\n",
        "\n",
        "    # if enable_query_cache and query_cache_file.is_file():\n",
        "    #     try:\n",
        "    #         with open(query_cache_file, \"rb\") as f:\n",
        "    #             query_cache = pickle.load(f)\n",
        "    #         if qhash in query_cache:\n",
        "    #             query_embedding_np = np.ascontiguousarray(np.asarray(query_cache[qhash], dtype=np.float32))\n",
        "    #     except Exception:\n",
        "    #         query_cache = {}\n",
        "\n",
        "    # if query_embedding_np is None: # Removed cache check\n",
        "    query_embedding = model.encode(\n",
        "        user_query,\n",
        "        convert_to_tensor=False,\n",
        "        normalize_embeddings=True,\n",
        "    )\n",
        "    query_embedding_np = np.ascontiguousarray(np.asarray(query_embedding, dtype=np.float32))\n",
        "    # Removed cache writing logic\n",
        "    # if enable_query_cache:\n",
        "    #     try:\n",
        "    #         query_cache[qhash] = query_embedding_np\n",
        "    #         raw = pickle.dumps(query_cache, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    #         _atomic_write_bytes(query_cache_file, raw)\n",
        "    #     except Exception:\n",
        "    #         pass\n",
        "\n",
        "    k = min(top_k, num_chunks)\n",
        "\n",
        "    # --- Search using FAISS when available, else numpy path ---\n",
        "    if faiss_index is not None:\n",
        "        try:\n",
        "            qv = np.ascontiguousarray(query_embedding_np.reshape(1, -1).astype(np.float32))\n",
        "            distances, indices = faiss_index.search(qv, k)\n",
        "            top_indices = indices[0].tolist()\n",
        "            top_scores = [float(d) for d in distances[0].tolist()]\n",
        "        except Exception:\n",
        "            # fallback to numpy\n",
        "            cos_scores = np.dot(chunk_embeddings_np, query_embedding_np)\n",
        "            if k == 1:\n",
        "                top_idx = int(np.argmax(cos_scores))\n",
        "                top_indices = [top_idx]\n",
        "                top_scores = [float(cos_scores[top_idx])]\n",
        "            else:\n",
        "                part = np.argpartition(-cos_scores, k - 1)[:k]\n",
        "                top_sorted = part[np.argsort(-cos_scores[part])]\n",
        "                top_indices = top_sorted.tolist()\n",
        "                top_scores = [float(cos_scores[i]) for i in top_indices]\n",
        "    else:\n",
        "        # FAISS not available: do exact dot-product\n",
        "        cos_scores = np.dot(chunk_embeddings_np, query_embedding_np)\n",
        "        if k == 1:\n",
        "            top_idx = int(np.argmax(cos_scores))\n",
        "            top_indices = [top_idx]\n",
        "            top_scores = [float(cos_scores[top_idx])]\n",
        "        else:\n",
        "            part = np.argpartition(-cos_scores, k - 1)[:k]\n",
        "            top_sorted = part[np.argsort(-cos_scores[part])]\n",
        "            top_indices = top_sorted.tolist()\n",
        "            top_scores = [float(cos_scores[i]) for i in top_indices]\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    for rank, (idx, score) in enumerate(zip(top_indices, top_scores), start=1):\n",
        "        results.append({\"rank\": rank, \"chunk_index\": int(idx), \"chunk\": chunks[int(idx)], \"score\": float(score)})\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_path = \"/content/drive/MyDrive/ColabContent/ch9.pdf\"\n",
        "    user_query = \"In Gulab Jamun chashni, which component is the solvent and which is the solute?\"\n",
        "\n",
        "    # Load model once and reuse (attempt GPU)\n",
        "\n",
        "\n",
        "    results = semantic_search_pdf(pdf_path, user_query, use_faiss=True, use_faiss_gpu=True)\n",
        "    for res in results:\n",
        "        print(f\"Rank: {res['rank']}, Score: {res['score']:.4f}, Chunk: {res['chunk']}\\n\")"
      ],
      "metadata": {
        "id": "Pwz2Xuvrj2oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "api_key = \"AIzaSyDDV2zSA9Ox0fYufWNvt-cbz1xx7IE0e0E\"\n",
        "\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "def answer_query(query: str, pdf_paths=None):\n",
        "    \"\"\"Answer a single query with Gemini using semantic search context.\"\"\"\n",
        "    if pdf_paths is None:\n",
        "        pdf_paths = [f\"./pdfs/ch{i}.pdf\" for i in range(1, 8)]\n",
        "\n",
        "    all_results = []\n",
        "    for pdf_path in pdf_paths:\n",
        "        try:\n",
        "            results = semantic_search_pdf(\n",
        "                user_query=query,\n",
        "                pdf_path=pdf_path\n",
        "            )\n",
        "            all_results.extend(results)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    # Sort by score and take top 5\n",
        "    all_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "    top_results = all_results[:5]\n",
        "\n",
        "    if top_results:\n",
        "        context_text = \"\\n\".join([res[\"chunk\"] for res in top_results])\n",
        "    else:\n",
        "        context_text = \"No relevant context found in the provided PDFs.\"\n",
        "\n",
        "    # Stream Gemini output\n",
        "    print(\"\\n🤖 Gemini Answer:\\n\")\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    response = model.generate_content(f\"{context_text}\\n\\nQuery: {query}\", stream=True)\n",
        "    for chunk in response:\n",
        "        if chunk.text:\n",
        "            print(chunk.text, end=\"\", flush=True)\n",
        "    print(\"\\n\")  # final newline\n"
      ],
      "metadata": {
        "id": "o9gDCC50mYot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_query(\"The book says that the curd you eat has bacteria in it.  Are these bacteria helpful or harmful? And what do they do to the milk to turn it into curd?\")"
      ],
      "metadata": {
        "id": "nD3l_y_QnMl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9z6up11L6hyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6600205f"
      },
      "source": [
        "%pip install Flask pyngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6fca068"
      },
      "source": [
        "from flask import Flask\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "\n",
        "# Setup Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return \"Hello, world!\"\n",
        "\n",
        "# Setup ngrok tunnel\n",
        "# Replace this with your authtoken\n",
        "ngrok.set_auth_token(\"31zvmOaz74YgWsb8mbEXEbrooAB_5kUzqT1CvGRV1PWa4bVfQ\") # Add your authtoken here\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(f\" * ngrok tunnel at {public_url}\")\n",
        "\n",
        "# Run Flask app in a separate thread\n",
        "threading.Thread(target=lambda: app.run(port=5000, use_reloader=False)).start()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}